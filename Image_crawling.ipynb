{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7yjrE4aECmY"
      },
      "source": [
        "# Image crawling by Selenium \n",
        "from google image, naver image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbtAkUVgEVaa"
      },
      "source": [
        "## importing library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L52CkJpUQXB",
        "outputId": "cb8e5b38-6a44-4416-8b18-1a3c1dec66f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n",
            "\u001b[K     |████████████████████████████████| 968 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 23.3 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 36.3 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.2.0)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.3 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [84.8 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,167 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "0% [15 Packages store 0 B] [7 InRelease gpgv 74.6 kB]"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "import urllib.request\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OGtyprFEkvn"
      },
      "source": [
        "## crawling code 함수화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAMFstk2EiyG"
      },
      "outputs": [],
      "source": [
        "def crawling_google(dog):\n",
        "  options = webdriver.ChromeOptions()\n",
        "  options.add_argument('--headless')        # Head-less 설정\n",
        "  options.add_argument('--no-sandbox')\n",
        "  options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver.Chrome('chromedriver', options=options)\n",
        "  driver.get(\"https://www.google.co.kr/imghp?hl=ko&tab=wi&authuser=0&ogbl\")\n",
        "  elem = driver.find_element_by_name(\"q\")\n",
        "  name = dog  # 검색할 검색어 입력\n",
        "  elem.send_keys(name)\n",
        "  elem.send_keys(Keys.RETURN)\n",
        "\n",
        "  SCROLL_PAUSE_TIME = 2\n",
        "  # Get scroll height\n",
        "  last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "  while True:\n",
        "      # Scroll down to bottom\n",
        "      driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "      # Wait to load page\n",
        "      time.sleep(SCROLL_PAUSE_TIME)\n",
        "      # Calculate new scroll height and compare with last scroll heigaht\n",
        "      new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "      if new_height == last_height:\n",
        "          try:\n",
        "              driver.find_element_by_css_selector(\".mye4qd\").click()\n",
        "          except:\n",
        "              break\n",
        "      last_height = new_height\n",
        "\n",
        "  images = driver.find_elements_by_css_selector(\".rg_i.Q4LuWd\")\n",
        "  count = 1\n",
        "  for image in images:\n",
        "    try:\n",
        "      image.click()\n",
        "      imgUrl = driver.find_element_by_xpath('/html/body/div[3]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[3]/div/a/img').get_attribute(\"src\")\n",
        "      opener=urllib.request.build_opener()\n",
        "      opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36')] # PC의 User-Agent값 입력\n",
        "      urllib.request.install_opener(opener)\n",
        "      urllib.request.urlretrieve(imgUrl, f\"/content/drive/MyDrive/KDT/offline/mini_project3-image/{dog}/google\"+str(count) + \".jpg\") # name 이름 폴더 생성후 해당 폴더에 이미지 저장\n",
        "      count = count + 1\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  driver.close()\n",
        "\n",
        "def crawling_naver(dog):\n",
        "  options = webdriver.ChromeOptions()\n",
        "  options.add_argument('--headless')  # Head-less 설정\n",
        "  options.add_argument('--no-sandbox')\n",
        "  options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver.Chrome('chromedriver', options=options) #chrome driver 설치한 경로 작성\n",
        "  driver.get(\"https://search.naver.com/search.naver?where=image&sm=tab_jum\") #구글 이미지 검색 url\n",
        "  element = WebDriverWait(driver, 5)  #켜질때의 대기시간\n",
        "  #암묵적 대기 -> webdriverwait : 켜질때까지 기다리고 parsing해야해서 이런처리하는것\n",
        "\n",
        "  elem = driver.find_element_by_name(\"query\") #구글검색창 선택\n",
        "  name = dog # 검색할 검색어 입력\n",
        "  elem.send_keys(name) #검색창에 검색할 내용 넣기\n",
        "  elem.send_keys(Keys.RETURN) #검색할 내용 넣고 enter 치는것\n",
        "\n",
        "\n",
        "  SCROLL_PAUSE_TIME = 2 #scroll 내려가는 시간\n",
        "  # Get scroll height\n",
        "  last_height = driver.execute_script(\"return document.body.scrollHeight\") #browser의 높이를 javascript로 찾음\n",
        "\n",
        "\n",
        "  while True:\n",
        "      # Scroll down to bottom\n",
        "      driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") #browser 끝까지 scroll 내림\n",
        "      # Wait to load page\n",
        "      time.sleep(SCROLL_PAUSE_TIME)\n",
        "      # Calculate new scroll height and compare with last scroll height\n",
        "      new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "      if new_height == last_height:\n",
        "          try:\n",
        "              driver.find_element_by_css_selector(\".mye4qd\").click() #'결과 더보기'가 뜨는 경우 이를 클릭해라\n",
        "          except:\n",
        "              break\n",
        "      last_height = new_height\n",
        "\n",
        "  images = driver.find_elements_by_css_selector(\"._image._listImage\")\n",
        "  count = 1\n",
        "  for image in images:\n",
        "      image.click()\n",
        "      imgUrl = driver.find_element_by_xpath(\"/html/body/div[3]/div[2]/div/div[1]/section[2]/div/div[2]/div/div/div[1]/div[1]/div/div/div[1]/div[1]/img\").get_attribute(\"src\")\n",
        "      opener=urllib.request.build_opener()\n",
        "      opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36')] # PC의 User-Agent값 입력\n",
        "      urllib.request.install_opener(opener)\n",
        "      urllib.request.urlretrieve(imgUrl, f\"/content/drive/MyDrive/KDT/offline/mini_project3-image/dog/{dog}/naver\"+str(count) + \".jpg\") # name 이름 폴더 생성후 해당 폴더에 이미지 저장\n",
        "      count = count + 1\n",
        "\n",
        "  driver.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEP_SJFNFbJN"
      },
      "source": [
        "맡은 강아지 품종 크롤링하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oazrzgAHFZbP"
      },
      "outputs": [],
      "source": [
        "crawling_google('웰시코기')\n",
        "crawling_naver('웰시코기')\n",
        "\n",
        "crawling_google('포메라니안')\n",
        "crawling_naver('포메라니안')\n",
        "\n",
        "crawling_google('리트리버')\n",
        "crawling_naver('리트리버')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
